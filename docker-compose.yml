version: '3.8'

x-airflow-common: &airflow-common
    build:
        context: ./airflow
        dockerfile: Dockerfile
    environment:
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@airflow-data:5432/airflow
        - AIRFLOW__CORE__EXECUTOR=LocalExecutor
        - AIRFLOW__CORE__LOAD_EXAMPLES=False
        - AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
    volumes:
        - ./airflow/dags:/opt/airflow/dags
        - ./airflow/plugins:/opt/airflow/plugins
        - ./airflow/logs:/opt/airflow/logs
        - ./airflow/includes:/opt/airflow/includes
        - ./airflow/config/config.ini:/opt/airflow/config.ini
        - ./spark/main:/opt/airflow/spark/main
        - ./spark/compressed_spark_jobs:/opt/airflow/spark/compressed_jobs
    networks:
        - stream-net

x-kafka-common: &kafka-common
    image: confluentinc/cp-kafka:7.6.5
    networks:
        - stream-net
    environment: &kafka-common-env
        KAFKA_PROCESS_ROLES: broker,controller
        KAFKA_CONTROLLER_QUORUM_VOTERS: "0@kafka-0:9093,1@kafka-1:9093,2@kafka-2:9093"
        CLUSTER_ID: "2qVoY3q1Q5O5hH1XVvqLmg"
        KAFKA_LISTENERS: "CONTROLLER://:9093,INTERNAL://:29092,DOCKER://:9092,EXTERNAL://0.0.0.0:9094"
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT,DOCKER:SASL_PLAINTEXT,EXTERNAL:SASL_PLAINTEXT"
        KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
        KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
        KAFKA_AUTO_CREATE_TOPICS_ENABLED: "true"
        KAFKA_NUM_PARTITIONS: 3
        KAFKA_SASL_ENABLED_MECHANISMS: "PLAIN"
        KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: "PLAIN"
        KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf"

services:
    kafka-0:
        <<: *kafka-common
        container_name: kafka-0
        hostname: kafka-0
        volumes:
            - kafka-data-0:/var/lib/kafka/data
            - ./kafka/config/kafka_server_jaas.conf:/etc/kafka/kafka_server_jaas.conf
        ports:
            - "9094:9094"
            - "9092:9092"
        environment:
            <<: *kafka-common-env
            KAFKA_NODE_ID: 0
            KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka-0:29092,DOCKER://kafka-0:9092,EXTERNAL://localhost:9094"

    kafka-1:
        <<: *kafka-common
        container_name: kafka-1
        hostname: kafka-1
        volumes:
            - kafka-data-1:/var/lib/kafka/data
            - ./kafka/config/kafka_server_jaas.conf:/etc/kafka/kafka_server_jaas.conf
        ports:
            - "9095:9094"
            - "9192:9092"
        environment:
            <<: *kafka-common-env
            KAFKA_NODE_ID: 1
            KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka-1:29092,DOCKER://kafka-1:9092,EXTERNAL://localhost:9095"

    kafka-2:
        <<: *kafka-common
        container_name: kafka-2
        hostname: kafka-2
        volumes:
            - kafka-data-2:/var/lib/kafka/data
            - ./kafka/config/kafka_server_jaas.conf:/etc/kafka/kafka_server_jaas.conf
        ports:
            - "9096:9094"
            - "9292:9092"
        environment:
            <<: *kafka-common-env
            KAFKA_NODE_ID: 2
            KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka-2:29092,DOCKER://kafka-2:9092,EXTERNAL://localhost:9096"

    akhq:
        image: tchiotludo/akhq:0.25.1
        container_name: akhq
        ports:
            - "8080:8080"
        networks:
            - stream-net
        environment:
            AKHQ_CONFIGURATION: |+
                akhq:
                    connections:
                      kafka-server:
                        properties:
                          bootstrap.servers: "kafka-0:9092,kafka-1:9092,kafka-2:9092"
                          security.protocol: SASL_PLAINTEXT
                          sasl.mechanism: PLAIN
                          sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="kafka" password="admin";
                    security:
                      default-group: no-roles
                      basic-auth:
                        - username: admin
                          password: "8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918"
                          passwordHash: SHA256
                          groups:
                            - admin
                micronaut:
                  security:
                    enabled: true
                    token:
                      jwt:
                        signatures:
                          secret:
                            generator:
                              secret: "6661a0975cd371c713f66a94bcd51ec874e0b040aec61f025964ad5fe7ee3120"

                            
    spark-master:
        build:
            context: ./spark
            dockerfile: Dockerfile
        container_name: spark-master
        command: bin/spark-class org.apache.spark.deploy.master.Master
        ports:
            - "7077:7077"
            - "8081:8080"
        environment:
            SPARK_MODE: master
            SPARK_RPC_AUTHENTICATION_ENABLED: no
            SPARK_RPC_ENCRYPTION_ENABLED: no
            SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
            SPARK_SSL_ENABLED: no
            SPARK_USER: spark
            SPARK_MASTER_OPTS: >
                -javaagent:/opt/jmx_exporter/jmx_prometheus_javaagent-0.18.0.jar=5556:/opt/jmx_exporter/config.yaml -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
        volumes:
            - spark_data:/data
            - ./spark/main:/opt/bitnami/spark/main
            - ./spark/compressed_spark_jobs:/opt/bitnami/spark/compressed_jobs
            - ./monitoring/jmx-exporter/configs/spark-jmx-config.yaml:/opt/jmx_exporter/config.yaml
            - ./monitoring/jmx-exporter/jmx_prometheus_javaagent-0.18.0.jar:/opt/jmx_exporter/jmx_prometheus_javaagent-0.18.0.jar
        networks:
            - stream-net

    spark-worker-1:
        build:
            context: ./spark
            dockerfile: Dockerfile
        container_name: spark-worker-1
        command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark-master:7077
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
            - SPARK_USER=spark
            - SPARK_MASTER_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
            - SPARK_DAEMON_JAVA_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
        depends_on:
            - spark-master
        volumes:
            - spark_data:/data
            - ./spark/main:/opt/bitnami/spark/main
            - ./spark/compressed_spark_jobs:/opt/bitnami/spark/compressed_jobs
        networks:
            - stream-net

    spark-worker-2:
        build:
            context: ./spark
            dockerfile: Dockerfile
        container_name: spark-worker-2
        command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark-master:7077
            - SPARK_WORKER_MEMORY=2G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
            - SPARK_USER=spark
            - SPARK_MASTER_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
            - SPARK_DAEMON_JAVA_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
        depends_on:
            - spark-master
        volumes:
            - spark_data:/data
            - ./spark/main:/opt/bitnami/spark/main
            - ./spark/compressed_spark_jobs:/opt/bitnami/spark/compressed_jobs
        networks:
            - stream-net

    airflow-data:
        image: postgres:16
        container_name: airflow-data
        hostname: airflow-data
        environment:
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=postgres
            - POSTGRES_DB=airflow
        healthcheck:
            test: [ "CMD-SHELL", "pg_isready -U postgres" ]
            interval: 10s
            timeout: 5s
            retries: 5
        volumes:
            - airflow_data:/var/lib/postgresql/data
        networks:
            - stream-net

    airflow-init:
        <<: *airflow-common
        container_name: airflow-init
        entrypoint: /bin/bash
        command: >
            -c 'airflow db init &&
                airflow users create \
                --role Admin \
                --username airflow \
                --password airflow \
                --email airflow@airflow.com \
                --firstname airflow \
                --lastname airflow;'
        depends_on:
            airflow-data:
                condition: service_healthy
        restart: on-failure

    airflow-webserver:
        <<: *airflow-common
        container_name: airflow-webserver
        command: airflow webserver
        ports:
            - "8082:8080"
        restart: unless-stopped
        depends_on:
            - airflow-init

    airflow-scheduler:
        <<: *airflow-common
        container_name: airflow-scheduler
        command: airflow scheduler
        restart: unless-stopped
        depends_on:
            - airflow-init
    mongodb:
        image: bitnami/mongodb:latest
        container_name: mongodb
        environment:
            - MONGODB_ROOT_USER=admin
            - MONGODB_ROOT_PASSWORD=admin
        ports:
            - "27018:27017"
        volumes:
            - mongo:/bitnami/mongo
            - ./mongo:/docker-entrypoint-initdb.d
        networks:
            - stream-net
    postgres:
        image: postgres:16
        hostname: postgres-data
        environment:
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=postgres
            - POSTGRES_DB=glamira
        ports:
            - "5432:5432"
        healthcheck:
            test: [ "CMD-SHELL", "pg_isready -U postgres" ]
            interval: 10s
            timeout: 5s
            retries: 5
        volumes:
            - postgres_data:/var/lib/postgresql/data
            - ./postgres/initdb:/docker-entrypoint-initdb.d
        networks:
            - stream-net
    pgadmin:
        image: dpage/pgadmin4
        environment:
            - PGADMIN_DEFAULT_EMAIL=postgresql@test.com
            - PGADMIN_DEFAULT_PASSWORD=postgresql
        ports:
            - "8085:80"
        networks:
            - stream-net
    # superset:
    #     image: apache/superset:latest
    #     container_name: superset
    #     environment:
    #         - SUPERSET_ENV=production
    #         - SUPERSET_PORT=8088
    #         - SUPERSET_USER=admin
    #         - SUPERSET_PASSWORD=admin
    #         - SUPERSET_DATABASE_URI=postgresql+psycopg2://admin:admin@dashboard_db:5433/dashboard
    #     ports:
    #         - "8088:8088"
    #     volumes:
    #         - ./superset:/app/superset
    #     networks:
    #         - stream-net

    prometheus:
        image: prom/prometheus:latest
        container_name: prometheus
        ports:
            - "9090:9090"
        volumes:
            - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
            - prometheus_data:/prometheus
        networks:
            - stream-net

    grafana:
        image: grafana/grafana:latest
        container_name: grafana
        ports:
            - "3000:3000"
        volumes:
            - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
            - grafana_data:/var/lib/grafana
        environment:
            GF_SECURITY_ADMIN_USER: admin
            GF_SECURITY_ADMIN_PASSWORD: admin
        networks:
            - stream-net

    jmx-exporter:
        image: bitnami/jmx-exporter:latest
        container_name: jmx-exporter
        command: >
            --jmx-url service:jmx:rmi:///jndi/rmi://spark-master:9999/jmxrmi --port 5556
        ports:
            - "5556:5556"
        networks:
            - stream-net

networks:
    stream-net:
        driver: bridge
        attachable: true

volumes:
    kafka-data-0:
    kafka-data-1:
    kafka-data-2:
    spark_data:
    airflow_data:
    mongo:
    postgres_data:
    prometheus_data:
    grafana_data:
